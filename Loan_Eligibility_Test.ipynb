{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Eligibility Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data sciencce beginner's project. We are using [Home loan data-set](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Labeled_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields and data types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select duplicate rows except first occurrence based on all columns\n",
    "duplicates = data[data.duplicated()]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values per column\n",
    "data.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking statistical values, specially 'std' for outliers\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "From the statistical values we've found that these Columns [ApplicantIncome, CoapplicantIncome, LoanAmount, \n",
    "Loan_Amount_Term] have very high standard deviation.\n",
    "'''\n",
    "\n",
    "data.boxplot(column=['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], figsize=(15,7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlation\n",
    "corr = data.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking target class distribution: Balance in dataset\n",
    "data.Loan_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_status values (Y/N) in ratio\n",
    "data.Loan_Status.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On plot\n",
    "ax = data.Loan_Status.value_counts().plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "- Small dataset containing both numerical and categorical values\n",
    "- No duplicates\n",
    "- Lots of missing values\n",
    "- There are extreme outliers in 'ApplicantIncome' and 'CoapplicantIncome' fields.\n",
    "- There is little positive correlation between LoanAmount and ApplicantIncome. \n",
    "- Data is highly imbalanced, (Y / N) distribution is approximately 2:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further exploration** of some particular fields to understand their role in the final outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking relation of Loan_Status (Y/N) with credit history(1 = meets guidelines / 0 = doesn't meet)\n",
    "\n",
    "hypothesis1 = data.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\n",
    "\n",
    "print ('Distribution of Loan status based on Credit History:\\n')\n",
    "print (hypothesis1)\n",
    "ax = hypothesis1.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People who have good credit history, most of the time they got their loan approaved. Which satisfy our **1st Hypothesis** (assumption). Now, let's look at the AplicantIncome and Loan_Status relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a scatter plot we can analyze the role of ApplicantIncome in outcome of Loan_Status\n",
    "ax1 = data.plot(kind='scatter', x='ApplicantIncome', y='Loan_Status', color='r', figsize=(15, 3))    \n",
    "\n",
    "print(ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the applicant's income does not play a significant role. We have to reject our **2nd Hypothesis**. But we can try binning applicant's incomes and check again their relation with Loan status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1000 $\n",
    "bins = [0, 5000, 10000, 15000, 25000, np.inf]\n",
    "names = ['< 5k', '5-10k', '10-15k', '15-25k', '25k+']\n",
    "\n",
    "data['IncomeRange'] = pd.cut(data['ApplicantIncome'], bins, labels=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis2 = pd.crosstab(data['IncomeRange'], data['Loan_Status'])\n",
    "hypothesis2.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that applicants with low or medium income, tend to apply more for loans and thus their number of applications are higher. However, the approval i.e. Loan Status (Y/N) seems not much influenced by Applicant's income and we also need to remember that our data (label) is highly imbalanced.\n",
    "Finally, we reject our **2nd Hypothesis** for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Checking influence of applicant's property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Property_Area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis3 = pd.crosstab(data['Property_Area'], data['Loan_Status'])\n",
    "hypothesis3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible that applicants with valuable property such as in the urban and semi-urban areas tend to get more loans, which satisfies our **3rd Hypothesis**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is a binary classification problem with many categorical values and also, we want to keep our base model simple, we will use Deccision Tree or Naive Byes classification algorithms to set a base standard for future comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates,so we can skip this step.\n",
    "\n",
    "#### Treating missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self_Employed column\n",
    "data['Self_Employed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since nearly 86% values are “No”, we can safely impute the missing values as “No”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self_Employed column has 32 missing values\n",
    "data['Self_Employed'].fillna('No',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat the missing values many different wasy such as droping the rows, filling with zeros or mean, as well as mode/median. Since we have categorical values, we can group by categories and take the median per group to fill loan amount missing values. So, we need a pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = data.pivot_table(values='LoanAmount', index='Self_Employed', columns='Education', aggfunc=np.median)\n",
    "\n",
    "# function to return value of this pivot_table\n",
    "def group_median(x):\n",
    "    return table.loc[x['Self_Employed'],x['Education']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values\n",
    "data['LoanAmount'].fillna(data[data['LoanAmount'].isnull()].apply(group_median, axis=1), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fill all the missing values in other columns with mode (highest number of occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Gender'].fillna(data['Gender'].mode()[0], inplace=True)\n",
    "data['Married'].fillna(data['Married'].mode()[0], inplace=True)\n",
    "data['Dependents'].fillna(data['Dependents'].mode()[0], inplace=True)\n",
    "data['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "data['Credit_History'].fillna(data['Credit_History'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking again for missing values\n",
    "data.apply(lambda x: sum(x.isnull()),axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treating extreme values with log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoanAmount\n",
    "data['LoanAmount_log'] = np.log(data['LoanAmount'])\n",
    "data['LoanAmount_log'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ApplicantIncome\n",
    "data['ApplicantIncome_log'] = np.log(data['ApplicantIncome'])\n",
    "data['ApplicantIncome_log'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type conversion: to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for numerical processing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "var_mod = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processed dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_data = data.drop('IncomeRange', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving the loan status column to the end since it is our label\n",
    "new_cols = [col for col in pro_data.columns if col != 'Loan_Status'] + ['Loan_Status']\n",
    "pro_data = pro_data[new_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's better to save the data\n",
    "pro_data.to_csv('processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries (Scikit learn)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the dataset into x (variables) and y (label). Also Loan_ID is not required for analysis\n",
    "x = pro_data.drop(['Loan_ID', 'Loan_Status'], axis = 1) \n",
    "y = pro_data[\"Loan_Status\"] \n",
    "print(x.shape) \n",
    "print(y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Split the data into training and testing sets \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "#Fit the model:\n",
    "dtc.fit(x_train, y_train)\n",
    "\n",
    "#Make predictions on training set:\n",
    "dtc_predictions = dtc.predict(x_test)\n",
    "\n",
    "#Print accuracy\n",
    "dtc_accuracy = metrics.accuracy_score(y_test, dtc_predictions)\n",
    "print (\"Accuracy of Decision Tree: %s\" % \"{0:.3%}\".format(dtc_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more on [Decision tree](https://www.datacamp.com/community/tutorials/decision-tree-classification-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "nbc = GaussianNB()\n",
    "\n",
    "#Fit the model:\n",
    "nbc.fit(x_train, y_train)\n",
    "\n",
    "#Make predictions on training set:\n",
    "nbc_predictions = nbc.predict(x_test)\n",
    "\n",
    "#Print accuracy\n",
    "nbc_accuracy = metrics.accuracy_score(y_test, nbc_predictions)\n",
    "print (\"Accuracy : %s\" % \"{0:.3%}\".format(nbc_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More resources on Naive Byes can be found [here](https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic improvements with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing 10-fold cross-validation (Rule of thumb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier()\n",
    "dtc_CV_scores = cross_val_score(clf1, x, y, cv=10)\n",
    "print(\"DT CV Accuracy: %0.2f (+/- %0.2f)\" % (dtc_CV_scores.mean(), dtc_CV_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = GaussianNB()\n",
    "nbc_CV_scores = cross_val_score(clf2, x, y, cv=10)\n",
    "print(\"NB CV Accuracy: %0.2f (+/- %0.2f)\" % (nbc_CV_scores.mean(), nbc_CV_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can do parameter optimization with **GridsearchCV** or **RandomSearch**. Also, we can try **ensemble** methods or Neural network. (...to be continued)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
